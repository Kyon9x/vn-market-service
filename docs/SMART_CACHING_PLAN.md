# Smart Caching Implementation Plan

## Overview
Comprehensive caching strategy to minimize vnstock API calls and prevent rate limiting with:
1. **Tiered Quote Caching** - Different TTLs based on asset type
2. **Incremental Historical Caching** - Store individual records, fetch only missing data
3. **Rate Limit Protection** - Queue and throttle API calls

---

## Problem Analysis

### Current Issues
1. **Quote Caching**: Fixed 5-minute TTL for all assets (inefficient for funds that change daily)
2. **Historical Caching**: Only exact date range matches work
   - Request 2025-10-01 to 2025-10-31 â†’ Cache miss â†’ API call
   - Request 2025-10-01 to 2025-11-01 â†’ Cache miss again â†’ Another API call (waste!)
3. **Rate Limiting**: 500 errors when too many API calls occur

### Root Cause
- No asset-specific quote TTL strategy
- No incremental historical data fetching
- No API rate limit protection

---

## Solution Architecture

### 1. Tiered Quote Caching Strategy

**Asset-Specific TTL Configuration:**

| Asset Type | Update Frequency | Cache TTL | Rationale |
|------------|-----------------|-----------|-----------|
| **FUND** | Once per day | 24 hours (86400s) | NAV updates once daily after market close |
| **STOCK** | Real-time during market | 1 hour (3600s) | Intraday quotes, but hourly is sufficient for most use cases |
| **INDEX** | Real-time during market | 1 hour (3600s) | Market indices update frequently but stable enough |
| **GOLD** | Multiple times per day | 1 hour (3600s) | Commodity prices change but not as volatile |
| **CRYPTO** | High volatility | 15 minutes (900s) | Future support for crypto assets |

**Benefits:**
- âœ… 96% reduction in fund quote API calls (5 min â†’ 24 hours)
- âœ… 92% reduction in stock/index/gold calls (5 min â†’ 1 hour)
- âœ… Appropriate freshness for each asset type
- âœ… User always gets relevant data

**Implementation:**
```python
QUOTE_TTL_CONFIG = {
    'FUND': 86400,      # 24 hours
    'STOCK': 3600,      # 1 hour
    'INDEX': 3600,      # 1 hour
    'GOLD': 3600,       # 1 hour
    'CRYPTO': 900,      # 15 minutes (future)
    'DEFAULT': 3600     # 1 hour fallback
}
```

---

### 2. Incremental Historical Data Caching

**New Database Schema:**

```sql
-- Individual historical records (never expire - historical data is immutable)
CREATE TABLE historical_records (
    symbol TEXT NOT NULL,
    asset_type TEXT NOT NULL,
    date TEXT NOT NULL,
    open REAL,
    high REAL,
    low REAL,
    close REAL,
    adjclose REAL,
    volume REAL,
    nav REAL,
    buy_price REAL,
    sell_price REAL,
    data_json TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, asset_type, date)
);

CREATE INDEX idx_historical_symbol_date ON historical_records(symbol, asset_type, date);
CREATE INDEX idx_historical_created ON historical_records(created_at);
```

**Smart Fetching Algorithm:**

```
Input: symbol, start_date, end_date, asset_type

Step 1: Query cached records
  SELECT date FROM historical_records 
  WHERE symbol = ? AND asset_type = ? 
  AND date BETWEEN ? AND ?

Step 2: Calculate missing dates
  requested_dates = [start_date..end_date]
  cached_dates = [dates from Step 1]
  missing_dates = requested_dates - cached_dates

Step 3: Smart fetch strategy
  IF missing_dates is empty:
    RETURN cached records
  
  IF missing_dates.length <= 7:
    # Fetch only missing dates to minimize API calls
    FOR each date_range in missing_dates:
      data = api.fetch(symbol, date_range)
      cache.store(data)
  
  ELSE IF missing_dates.length > 30:
    # Large gap - fetch all at once (more efficient)
    data = api.fetch(symbol, start_date, end_date)
    cache.store(data)
  
  ELSE:
    # Medium gap - batch missing dates
    data = api.fetch(symbol, min(missing_dates), max(missing_dates))
    cache.store(data)

Step 4: Merge and return
  cached_data = fetch cached records
  new_data = fetch new records
  RETURN merge(cached_data, new_data).sort_by_date()
```

**Example Scenarios:**

**Scenario A: Extending date range**
```
Day 1: User requests VNDBF 2025-10-01 to 2025-10-31
  - Cache: empty
  - Fetch: 2025-10-01 to 2025-10-31 (31 days)
  - Store: 31 records
  - API calls: 1

Day 2: User requests VNDBF 2025-10-01 to 2025-11-01
  - Cache: 31 records (Oct 1-31)
  - Missing: 2025-11-01 (1 day)
  - Fetch: 2025-11-01 only
  - Store: 1 new record
  - API calls: 1
  
Result: 97% reduction (fetch 1 day instead of 32)
```

**Scenario B: Overlapping ranges**
```
Day 1: User A requests VNM 2025-10-01 to 2025-10-15
  - Fetch: 15 days
  - Store: 15 records
  - API calls: 1

Day 1: User B requests VNM 2025-10-10 to 2025-10-25
  - Cache: 6 records (Oct 10-15)
  - Missing: 10 records (Oct 16-25)
  - Fetch: 2025-10-16 to 2025-10-25 only
  - Store: 10 new records
  - API calls: 1

Result: 40% reduction (fetch 10 days instead of 16)
```

**Scenario C: Today's data auto-fill**
```
User searches symbol: VNM
  - Fetch quote for VNM (normal flow)
  - Auto-store: Today's quote as historical record
  - Next request for recent history â†’ already cached

Benefit: Zero additional API calls for recent history
```

---

### 3. Rate Limit Protection

**Rate Limiter Component:**

```python
class RateLimitProtector:
    """Protect against API rate limits with throttling and queuing."""
    
    Config:
        - max_calls_per_minute: 60
        - max_calls_per_hour: 500
        - delay_between_calls: 100ms
        - queue_max_size: 100
    
    Features:
        - Track API call timestamps
        - Enforce delays between calls
        - Queue requests when at limit
        - Return cached data as fallback
        - Log rate limit warnings
```

**Integration Pattern:**

```python
# Before API call
if rate_limiter.should_throttle():
    # Try cache first
    cached_data = cache.get(...)
    if cached_data:
        logger.warning("Rate limit reached, returning cached data")
        return cached_data
    
    # Wait or queue
    rate_limiter.wait_for_slot()

# Make API call
result = api.call(...)
rate_limiter.record_call()
```

---

## Implementation Plan

### Phase 1: Core Infrastructure (Priority: HIGH)

#### Task 1.1: Create Historical Records Table
**File:** `app/cache/migrations.py` (new)
```python
def migrate_historical_records():
    """Create historical_records table for incremental caching."""
```

#### Task 1.2: Implement Quote TTL Manager
**File:** `app/cache/quote_ttl_manager.py` (new)
```python
class QuoteTTLManager:
    def get_ttl_for_asset(self, asset_type: str) -> int
    def should_refresh_quote(self, symbol: str, asset_type: str) -> bool
```

#### Task 1.3: Build Historical Cache Manager
**File:** `app/cache/historical_cache.py` (new)
```python
class HistoricalCacheManager:
    def get_cached_dates(self, symbol, start, end, asset_type)
    def get_missing_date_ranges(self, requested, cached)
    def store_historical_records(self, symbol, asset_type, records)
    def get_historical_data_smart(self, symbol, start, end, asset_type)
```

#### Task 1.4: Create Rate Limit Protector
**File:** `app/cache/rate_limit_protector.py` (new)
```python
class RateLimitProtector:
    def should_throttle(self) -> bool
    def wait_for_slot(self) -> None
    def record_call(self) -> None
    def get_stats(self) -> Dict
```

### Phase 2: Client Integration (Priority: HIGH)

#### Task 2.1: Update StockClient
**File:** `app/clients/stock_client.py`
- Line 64: Update `get_latest_quote()` to use asset-specific TTL
- Line 25: Update `get_stock_history()` to use incremental caching

#### Task 2.2: Update FundClient
**File:** `app/clients/fund_client.py`
- Update quote methods with 24-hour TTL
- Update history methods with incremental caching

#### Task 2.3: Update IndexClient
**File:** `app/clients/index_client.py`
- Update quote methods with 1-hour TTL
- Update history methods with incremental caching

#### Task 2.4: Update GoldClient
**File:** `app/clients/gold_client.py`
- Line 134: Update `get_gold_history()` to use incremental caching
- Update quote methods with 1-hour TTL

### Phase 3: Background Tasks (Priority: MEDIUM)

#### Task 3.1: Auto-fill Today's Data
**File:** `app/cache/background_manager.py`
```python
async def auto_fill_todays_quote(symbol, asset_type, quote_data):
    """Automatically store today's quote as historical record."""
```

#### Task 3.2: Pre-cache Popular Assets
**File:** `app/cache/background_manager.py`
```python
async def refresh_popular_historical_data():
    """Daily refresh of historical data for top 100 symbols."""
```

#### Task 3.3: Smart Quote Refresh
**File:** `app/cache/background_manager.py`
```python
async def smart_quote_refresh():
    """Refresh quotes based on asset-specific TTL."""
```

### Phase 4: Testing & Monitoring (Priority: HIGH)

#### Task 4.1: Unit Tests
- Test incremental fetching logic
- Test TTL configuration
- Test rate limit protection
- Test date range calculations

#### Task 4.2: Integration Tests
- Test end-to-end historical data flow
- Test quote caching with different asset types
- Test rate limit scenarios

#### Task 4.3: Add Monitoring
**File:** `app/main.py` - New endpoints
```python
@app.get("/cache/stats/historical")
@app.get("/cache/stats/rate-limits")
```

---

## Expected Performance Improvements

### Quote Caching
| Asset Type | Before | After | Improvement |
|------------|--------|-------|-------------|
| Funds | 288 calls/day | 1 call/day | **99.7%** reduction |
| Stocks | 288 calls/day | 24 calls/day | **91.7%** reduction |
| Indices | 288 calls/day | 24 calls/day | **91.7%** reduction |
| Gold | 288 calls/day | 24 calls/day | **91.7%** reduction |

### Historical Data Caching
| Scenario | Before | After | Improvement |
|----------|--------|-------|-------------|
| Same range twice | 2 API calls | 1 API call | **50%** reduction |
| Extended range | 2 full calls | 1 full + 1 partial | **70-90%** reduction |
| Overlapping ranges | Multiple full calls | Partial fetches only | **80-95%** reduction |

### Overall Impact
- **API Call Reduction**: 85-95% overall
- **Response Time**: 5-10ms (cached) vs 500-2000ms (API)
- **Rate Limit Errors**: Eliminated
- **Database Growth**: ~1KB per symbol per day (~365KB/symbol/year)

---

## File Structure

```
app/cache/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ cache_manager.py              # Enhanced with new methods
â”œâ”€â”€ memory_cache.py               # Enhanced QuoteCache with TTL config
â”œâ”€â”€ historical_cache.py           # NEW: Incremental historical fetching
â”œâ”€â”€ quote_ttl_manager.py          # NEW: Asset-specific TTL management
â”œâ”€â”€ rate_limit_protector.py       # NEW: Rate limit protection
â”œâ”€â”€ background_manager.py         # Enhanced with new tasks
â”œâ”€â”€ data_seeder.py                # Enhanced with historical seeding
â”œâ”€â”€ migrations.py                 # NEW: Database migrations
â””â”€â”€ search_optimizer.py           # Existing

app/clients/
â”œâ”€â”€ stock_client.py               # UPDATE: Use new caching
â”œâ”€â”€ fund_client.py                # UPDATE: Use new caching
â”œâ”€â”€ index_client.py               # UPDATE: Use new caching
â””â”€â”€ gold_client.py                # UPDATE: Use new caching

app/
â”œâ”€â”€ main.py                       # UPDATE: Add new endpoints
â””â”€â”€ config.py                     # UPDATE: Add cache configuration
```

---

## Configuration

**File:** `app/config.py`
```python
# Quote Cache TTL Configuration (seconds)
QUOTE_TTL_CONFIG = {
    'FUND': 86400,      # 24 hours - NAV updates daily
    'STOCK': 3600,      # 1 hour - frequent updates but hourly sufficient
    'INDEX': 3600,      # 1 hour - market index updates
    'GOLD': 3600,       # 1 hour - commodity price updates
    'CRYPTO': 900,      # 15 minutes - high volatility (future)
    'DEFAULT': 3600     # 1 hour - default fallback
}

# Historical Cache Configuration
HISTORICAL_CACHE_CONFIG = {
    'max_gap_for_partial_fetch': 7,    # Days
    'min_gap_for_full_fetch': 30,      # Days
    'auto_fill_today': True,            # Auto-fill today's quote
    'never_expire': True                # Historical data never expires
}

# Rate Limit Configuration
RATE_LIMIT_CONFIG = {
    'max_calls_per_minute': 60,
    'max_calls_per_hour': 500,
    'delay_between_calls_ms': 100,
    'queue_max_size': 100,
    'enable_throttling': True
}
```

---

## Migration Steps

### Step 1: Database Migration
```bash
# Run migration script
python -m app.cache.migrations
```

### Step 2: Deploy New Cache Components
```bash
# Deploy updated code
git pull
docker-compose up --build -d
```

### Step 3: Seed Historical Data (Optional)
```bash
# Pre-populate cache with historical data for popular symbols
curl -X POST http://localhost:8000/cache/seed/historical
```

### Step 4: Monitor Performance
```bash
# Check cache statistics
curl http://localhost:8000/cache/stats
curl http://localhost:8000/cache/stats/historical
curl http://localhost:8000/cache/stats/rate-limits
```

---

## Rollback Plan

If issues occur:

1. **Disable new features via config:**
```python
HISTORICAL_CACHE_CONFIG['enable_incremental'] = False
RATE_LIMIT_CONFIG['enable_throttling'] = False
```

2. **Revert to old cache behavior:**
```python
# Clients will fall back to old get_historical_data()
```

3. **Database rollback:**
```sql
-- Drop new table if needed
DROP TABLE IF EXISTS historical_records;
```

---

## Testing Checklist

### Unit Tests
- [ ] Quote TTL manager returns correct TTL for each asset type
- [ ] Historical cache correctly identifies missing dates
- [ ] Date range calculations are accurate
- [ ] Rate limiter correctly tracks and throttles calls
- [ ] Merge logic combines cached and new data correctly

### Integration Tests
- [ ] End-to-end quote caching with different asset types
- [ ] Historical data incremental fetching works correctly
- [ ] Rate limit protection prevents 500 errors
- [ ] Background tasks execute without errors
- [ ] Auto-fill today's data works correctly

### Performance Tests
- [ ] Quote response time < 10ms (cached)
- [ ] Historical data response time < 50ms (partial cache)
- [ ] API call reduction >= 85%
- [ ] Rate limit never exceeded
- [ ] Database size growth is acceptable

### User Acceptance Tests
- [ ] Users get fresh fund quotes (daily updates)
- [ ] Users get reasonably fresh stock quotes (hourly)
- [ ] Historical data requests complete without errors
- [ ] No 500 errors from rate limiting
- [ ] Response times are improved

---

## Success Metrics

### Key Performance Indicators (KPIs)

1. **API Call Reduction**
   - Target: 85-95% reduction
   - Measure: Compare API calls before/after per hour

2. **Cache Hit Rate**
   - Quotes: Target 90%+ for funds, 80%+ for stocks
   - Historical: Target 85%+

3. **Error Rate**
   - Rate limit 500 errors: Target 0
   - Cache errors: Target < 0.1%

4. **Response Time**
   - Cached quotes: Target < 10ms
   - Cached history: Target < 50ms
   - Partial history fetch: Target < 200ms

5. **Database Size**
   - Growth rate: ~1KB/symbol/day
   - Total size after 1 year: < 500MB for 1000 symbols

---

## Weekend/Market Holiday Fallback Implementation

### Problem
On weekends and market holidays, the `/quote/{symbol}` endpoint would return 404 errors because the vnstock API returns no data for the current date.

### Solution: 3-Tier Fallback System

**Fallback Logic Flow:**
```
Step 1: Try API call for today's quote
  â†“ (if empty or fails)
Step 2: Check historical cache for most recent record (within 30 days)
  â†“ (if no cache found)
Step 3: Fetch last 7 days of data, cache it, return most recent record
```

**Implementation Status:** âœ… **COMPLETE**

### Changes Made

#### 1. Added Symbol Field to Historical Records
**Issue:** Historical records returned by history fetch methods were missing the `symbol` field, causing Pydantic validation errors when used as fallback quotes.

**Files Modified:**
- **`app/clients/stock_client.py:133`** - Added `"symbol": symbol` to history records
- **`app/clients/stock_client.py:207-209`** - Added symbol check in fallback #2
- **`app/clients/fund_client.py:252`** - Added `"symbol": symbol` to fund NAV history records
- **`app/clients/fund_client.py:378-383`** - Added symbol field to fallback records
- **`app/clients/index_client.py:105`** - Added `"symbol": symbol` to index history records
- **`app/clients/index_client.py:183-185`** - Added symbol check in fallback #2
- **`app/clients/gold_client.py:123`** - Added `"symbol": "VN.GOLD.SJC"` to SJC history
- **`app/clients/gold_client.py:178`** - Added `"symbol": "VN.GOLD.BTMC"` to BTMC history
- **`app/clients/gold_client.py:227`** - Added `"symbol": "GOLD.MSN"` to MSN history
- **`app/clients/gold_client.py:311-313`** - Added symbol check in fallback #2

#### 2. Fixed Null Reference Bug
**Issue:** `main.py:922` was calling `fund_client.get_funds_list()` without checking if `fund_client` is `None`, causing crashes when fund API initialization fails.

**Fix:** Added null check at `app/main.py:922`
```python
if fund_client:
    fund_symbols = [f["symbol"] for f in fund_client.get_funds_list()]
```

### Test Results (Nov 2, 2025 - Sunday)

All weekend fallback tests passed successfully:

```bash
# Stock quotes return last trading day (Friday Oct 31)
curl http://localhost:8765/quote/FPT
# âœ… Returns: 200 OK, date: "2025-10-31", close: 103900.0

curl http://localhost:8765/quote/VNM  
# âœ… Returns: 200 OK, date: "2025-10-31", close: 57600.0

# Index quotes return last trading day
curl http://localhost:8765/quote/VNINDEX
# âœ… Returns: 200 OK, date: "2025-10-31", close: 1639.65

# Gold has real-time pricing even on weekends
curl http://localhost:8765/quote/VN.GOLD.SJC
# âœ… Returns: 200 OK, date: "2025-11-02", close: 148400000.0
```

**Logs Confirm Fallback Logic:**
```
INFO: API call failed for FPT: RetryError..., will try fallback
INFO: Using last week's most recent data for FPT from 2025-10-31
```

### Benefits
- âœ… **No 404 errors** on weekends/holidays
- âœ… **Users always get data** - Most recent available quote
- âœ… **No API changes needed** - Fully backward compatible
- âœ… **Automatic caching** - Fallback data is cached for future requests
- âœ… **Works across all asset types** - Stocks, funds, indices, gold

### Edge Cases Handled
1. **Market closed days** - Returns last trading day's data
2. **API failures** - Falls back to cached historical data
3. **New symbols** - Fetches last week's data to populate cache
4. **Multiple consecutive holidays** - Returns most recent available data within 30-day window

---

## Next Steps

**Immediate Actions:**
1. Review and approve this plan
2. Prioritize Phase 1 tasks
3. Set up development environment
4. Begin implementation

**Implementation Timeline:**
- Phase 1: 2-3 hours (core infrastructure)
- Phase 2: 2-3 hours (client integration)
- Phase 3: 1-2 hours (background tasks)
- Phase 4: 2-3 hours (testing & monitoring)

**Total Estimated Time:** 7-11 hours

---

## Questions & Decisions Needed

1. **TTL Configuration**: Are the proposed TTLs acceptable?
   - Fund: 24 hours
   - Stock/Index/Gold: 1 hour
   - Crypto: 15 minutes

2. **Rate Limits**: What are vnstock's actual rate limits?
   - Currently guessed: 60/min, 500/hour

3. **Storage**: Is ~500MB database size acceptable for 1 year?

4. **Background Tasks**: Should we pre-cache popular assets?
   - Pros: Faster response, fewer runtime API calls
   - Cons: More background API calls

5. **Crypto Support**: Should we implement crypto caching now or later?

---

## Implementation Status

### âœ… Completed Features
- **Weekend/Market Holiday Fallback** - 3-tier fallback system fully operational
  - All client files updated with symbol field in historical records
  - Null reference bug fixed in main.py
  - Tested and working for stocks, funds, indices, and gold
  - See "Weekend/Market Holiday Fallback Implementation" section above

### ðŸ”„ In Progress
- None

### ðŸ“‹ Pending
- Phase 1: Core Infrastructure (quote TTL, historical cache, rate limiter)
- Phase 2: Client Integration (asset-specific TTL implementation)
- Phase 3: Background Tasks (auto-fill, pre-caching)
- Phase 4: Testing & Monitoring (unit tests, integration tests, metrics endpoints)

---

**Status:** ðŸ“‹ Plan Ready for Review and Implementation | âœ… Weekend Fallback Complete
